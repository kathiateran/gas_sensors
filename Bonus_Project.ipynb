{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.799_0.201</th>\n",
       "      <th>0.799_0.201.1</th>\n",
       "      <th>0.700_0.300</th>\n",
       "      <th>0.700_0.300.1</th>\n",
       "      <th>0.600_0.400</th>\n",
       "      <th>0.600_0.400.1</th>\n",
       "      <th>0.501_0.499</th>\n",
       "      <th>0.501_0.499.1</th>\n",
       "      <th>0.400_0.600</th>\n",
       "      <th>0.400_0.600.1</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-11.82</td>\n",
       "      <td>-13.29</td>\n",
       "      <td>-19.32</td>\n",
       "      <td>-26.28</td>\n",
       "      <td>-38.14</td>\n",
       "      <td>-50.09</td>\n",
       "      <td>-59.78</td>\n",
       "      <td>-75.04</td>\n",
       "      <td>-85.63</td>\n",
       "      <td>-104.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-11.54</td>\n",
       "      <td>-14.18</td>\n",
       "      <td>-25.35</td>\n",
       "      <td>-32.75</td>\n",
       "      <td>-48.77</td>\n",
       "      <td>-60.08</td>\n",
       "      <td>-75.41</td>\n",
       "      <td>-89.72</td>\n",
       "      <td>-104.25</td>\n",
       "      <td>-121.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-12.45</td>\n",
       "      <td>-15.81</td>\n",
       "      <td>-33.33</td>\n",
       "      <td>-40.64</td>\n",
       "      <td>-61.50</td>\n",
       "      <td>-72.12</td>\n",
       "      <td>-91.75</td>\n",
       "      <td>-104.67</td>\n",
       "      <td>-124.21</td>\n",
       "      <td>-139.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-14.67</td>\n",
       "      <td>-18.49</td>\n",
       "      <td>-40.56</td>\n",
       "      <td>-47.67</td>\n",
       "      <td>-72.11</td>\n",
       "      <td>-82.19</td>\n",
       "      <td>-106.26</td>\n",
       "      <td>-118.26</td>\n",
       "      <td>-141.95</td>\n",
       "      <td>-155.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-18.07</td>\n",
       "      <td>-19.74</td>\n",
       "      <td>-47.08</td>\n",
       "      <td>-53.50</td>\n",
       "      <td>-81.15</td>\n",
       "      <td>-89.81</td>\n",
       "      <td>-117.91</td>\n",
       "      <td>-128.10</td>\n",
       "      <td>-156.66</td>\n",
       "      <td>-168.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>-66.05</td>\n",
       "      <td>-55.55</td>\n",
       "      <td>-136.84</td>\n",
       "      <td>-112.44</td>\n",
       "      <td>-211.41</td>\n",
       "      <td>-176.55</td>\n",
       "      <td>-300.14</td>\n",
       "      <td>-252.75</td>\n",
       "      <td>-417.08</td>\n",
       "      <td>-353.50</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>-73.04</td>\n",
       "      <td>-60.95</td>\n",
       "      <td>-141.43</td>\n",
       "      <td>-116.96</td>\n",
       "      <td>-217.81</td>\n",
       "      <td>-183.10</td>\n",
       "      <td>-309.05</td>\n",
       "      <td>-261.66</td>\n",
       "      <td>-429.28</td>\n",
       "      <td>-365.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>-76.09</td>\n",
       "      <td>-63.93</td>\n",
       "      <td>-143.70</td>\n",
       "      <td>-119.61</td>\n",
       "      <td>-221.81</td>\n",
       "      <td>-187.00</td>\n",
       "      <td>-313.96</td>\n",
       "      <td>-266.76</td>\n",
       "      <td>-436.02</td>\n",
       "      <td>-372.47</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>-78.81</td>\n",
       "      <td>-65.68</td>\n",
       "      <td>-146.01</td>\n",
       "      <td>-121.49</td>\n",
       "      <td>-223.79</td>\n",
       "      <td>-189.51</td>\n",
       "      <td>-319.61</td>\n",
       "      <td>-271.93</td>\n",
       "      <td>-439.91</td>\n",
       "      <td>-376.66</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>-82.17</td>\n",
       "      <td>-67.91</td>\n",
       "      <td>-149.03</td>\n",
       "      <td>-123.40</td>\n",
       "      <td>-225.73</td>\n",
       "      <td>-191.28</td>\n",
       "      <td>-322.75</td>\n",
       "      <td>-274.53</td>\n",
       "      <td>-447.74</td>\n",
       "      <td>-381.94</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.799_0.201  0.799_0.201.1  0.700_0.300  0.700_0.300.1  0.600_0.400  \\\n",
       "0         -11.82         -13.29       -19.32         -26.28       -38.14   \n",
       "1         -11.54         -14.18       -25.35         -32.75       -48.77   \n",
       "2         -12.45         -15.81       -33.33         -40.64       -61.50   \n",
       "3         -14.67         -18.49       -40.56         -47.67       -72.11   \n",
       "4         -18.07         -19.74       -47.08         -53.50       -81.15   \n",
       "..           ...            ...          ...            ...          ...   \n",
       "120       -66.05         -55.55      -136.84        -112.44      -211.41   \n",
       "121       -73.04         -60.95      -141.43        -116.96      -217.81   \n",
       "122       -76.09         -63.93      -143.70        -119.61      -221.81   \n",
       "123       -78.81         -65.68      -146.01        -121.49      -223.79   \n",
       "124       -82.17         -67.91      -149.03        -123.40      -225.73   \n",
       "\n",
       "     0.600_0.400.1  0.501_0.499  0.501_0.499.1  0.400_0.600  0.400_0.600.1  \\\n",
       "0           -50.09       -59.78         -75.04       -85.63        -104.66   \n",
       "1           -60.08       -75.41         -89.72      -104.25        -121.90   \n",
       "2           -72.12       -91.75        -104.67      -124.21        -139.76   \n",
       "3           -82.19      -106.26        -118.26      -141.95        -155.84   \n",
       "4           -89.81      -117.91        -128.10      -156.66        -168.72   \n",
       "..             ...          ...            ...          ...            ...   \n",
       "120        -176.55      -300.14        -252.75      -417.08        -353.50   \n",
       "121        -183.10      -309.05        -261.66      -429.28        -365.93   \n",
       "122        -187.00      -313.96        -266.76      -436.02        -372.47   \n",
       "123        -189.51      -319.61        -271.93      -439.91        -376.66   \n",
       "124        -191.28      -322.75        -274.53      -447.74        -381.94   \n",
       "\n",
       "     Class  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "..     ...  \n",
       "120      4  \n",
       "121      4  \n",
       "122      4  \n",
       "123      4  \n",
       "124      4  \n",
       "\n",
       "[125 rows x 11 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('/Users/kathiateran/Documents/Machine Learning/A1. FINAL/final_bonus.csv', sep=',')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 0:10].values\n",
    "y = dataset.iloc[:, 10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most significant features are:\n",
      "\n",
      " 0.7910828010516047 1.0 0.6008128179935973 0.6902062425439259 0.4962726276805533 0.5336396751236808 0.4718288588956607 0.48364608528754766 0.45955982092279546 0.46233693132963566\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Univariate feature selection with F-test for feature scoring\n",
    "# We use the default selection function to select the 10\n",
    "# most significant features\n",
    "X_indices = np.arange(X.shape[-1])\n",
    "\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "selector.fit(X_train, y_train)\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "scores /= scores.max()\n",
    "plt.bar(X_indices - .45, scores, width=.2,\n",
    "        label=r'Univariate score ($-Log(p_{value})$)', color='darkorange',\n",
    "        edgecolor='black')\n",
    "\n",
    "scorelist = []\n",
    "scorelist.append(scores)\n",
    "\n",
    "for elem in scorelist:\n",
    "    print ('The 10 most significant features are:\\n\\n', *elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy without selecting features:\n",
      " 80.00%\n",
      "\n",
      "Prediction accuracy for SVM:\n",
      "80.00%\n",
      "\n",
      "Confusion Matrix\n",
      "[[7 0 0 0 0]\n",
      " [0 3 0 0 0]\n",
      " [0 2 4 0 0]\n",
      " [0 3 0 1 0]\n",
      " [0 0 0 0 5]]\n",
      "\n",
      "\n",
      "Accuracy [1.   0.8  0.92 0.88 1.  ]\n",
      "True Positive Rate [1.         1.         0.66666667 0.25       1.        ]\n",
      "Specificity [1.         0.77272727 1.         1.         1.        ]\n",
      "False Positive Rate [0.         0.22727273 0.         0.         0.        ]\n",
      "False Negative Rate [0.         0.         0.33333333 0.75       0.        ]\n",
      "\n",
      "\n",
      "Average Accuracy 92.00%\n",
      "Average True Positive Rate 78.33%\n",
      "Average Specificity 95.45%\n",
      "Average False Positive Rate 4.55%\n",
      "Average False Negative Rate 21.67%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from hmmlearn import hmm\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from fcmeans import FCM\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "# Compare to the weights of an SVM\n",
    "clf = svm.SVC(kernel='linear', C=1.0, gamma='auto')\n",
    "clf.fit(X_train, y_train)\n",
    "print('Classification accuracy without selecting features:\\n', '{:.2%}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "svm = make_pipeline(SelectKBest(f_classif, k=10), svm.SVC(kernel='linear', C=1.0, gamma='auto'))\n",
    "\n",
    "svm_fit = svm.fit(X_train, y_train)\n",
    "pred_test_svm = svm.predict(X_test)\n",
    "\n",
    "\n",
    "print('\\nPrediction accuracy for SVM:')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_svm)))\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(metrics.confusion_matrix(y_test, pred_test_svm))\n",
    "\n",
    "cm=confusion_matrix(y_test, pred_test_svm)\n",
    "\n",
    "print(\"\\n\")\n",
    "        \n",
    "FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "TPR = TP/(TP+FN)\n",
    "TNR = TN/(TN+FP) \n",
    "FPR = FP/(FP+TN)\n",
    "FNR = FN/(TP+FN)\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "        \n",
    "print(\"Accuracy\", ACC)\n",
    "print(\"True Positive Rate\", TPR)\n",
    "print(\"Specificity\", TNR)\n",
    "print(\"False Positive Rate\", FPR)\n",
    "print(\"False Negative Rate\", FNR) \n",
    "print(\"\\n\")  \n",
    "\n",
    "ACCavg = np.mean(ACC)\n",
    "TPRavg = np.mean(TPR)\n",
    "TNRavg = np.mean(TNR)\n",
    "FPRavg = np.mean(FPR)\n",
    "FNRavg = np.mean(FNR)\n",
    "            \n",
    "print(\"Average Accuracy\", '{:.2%}'.format(ACCavg))\n",
    "print(\"Average True Positive Rate\", '{:.2%}'.format(TPRavg))\n",
    "print(\"Average Specificity\", '{:.2%}'.format(TNRavg))\n",
    "print(\"Average False Positive Rate\", '{:.2%}'.format(FPRavg))\n",
    "print(\"Average False Negative Rate\", '{:.2%}'.format(FNRavg))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy without selecting features:\n",
      " 76.00%\n",
      "Prediction accuracy for KNN\n",
      "76.00%\n",
      "\n",
      "Confusion Matrix\n",
      "[[0 0 2 0 5]\n",
      " [0 0 2 1 0]\n",
      " [0 2 0 4 0]\n",
      " [2 0 0 1 1]\n",
      " [0 0 4 1 0]]\n",
      "\n",
      "\n",
      "Accuracy [1.   1.   0.76 0.88 0.88]\n",
      "True Positive Rate [1.   1.   1.   0.25 0.4 ]\n",
      "Specificity [1.         1.         0.68421053 1.         1.        ]\n",
      "False Positive Rate [0.         0.         0.31578947 0.         0.        ]\n",
      "False Negative Rate [0.   0.   0.   0.75 0.6 ]\n",
      "\n",
      "\n",
      "Average Accuracy 90.40%\n",
      "Average True Positive Rate 73.00%\n",
      "Average Specificity 93.68%\n",
      "Average False Positive Rate 6.32%\n",
      "Average False Negative Rate 27.00%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare to the weights of an Knn\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "\n",
    "knn_clf.fit(X_train, y_train)\n",
    "print('Classification accuracy without selecting features:\\n', '{:.2%}'.format(knn_clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "knn = make_pipeline(SelectKBest(f_classif, k=10), MinMaxScaler(), KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2))\n",
    "\n",
    "knn_fit = knn.fit(X_train, y_train)\n",
    "pred_test_knn = knn.predict(X_test)\n",
    "\n",
    "print('Prediction accuracy for KNN')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_knn)))\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(metrics.confusion_matrix(y_test, pred_test_hmm))\n",
    "\n",
    "cm=confusion_matrix(y_test, pred_test_knn)\n",
    "\n",
    "print(\"\\n\")\n",
    "        \n",
    "FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "TPR = TP/(TP+FN)\n",
    "TNR = TN/(TN+FP) \n",
    "FPR = FP/(FP+TN)\n",
    "FNR = FN/(TP+FN)\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "        \n",
    "print(\"Accuracy\", ACC)\n",
    "print(\"True Positive Rate\", TPR)\n",
    "print(\"Specificity\", TNR)\n",
    "print(\"False Positive Rate\", FPR)\n",
    "print(\"False Negative Rate\", FNR) \n",
    "print(\"\\n\")  \n",
    "\n",
    "ACCavg = np.mean(ACC)\n",
    "TPRavg = np.mean(TPR)\n",
    "TNRavg = np.mean(TNR)\n",
    "FPRavg = np.mean(FPR)\n",
    "FNRavg = np.mean(FNR)\n",
    "            \n",
    "print(\"Average Accuracy\", '{:.2%}'.format(ACCavg))\n",
    "print(\"Average True Positive Rate\", '{:.2%}'.format(TPRavg))\n",
    "print(\"Average Specificity\", '{:.2%}'.format(TNRavg))\n",
    "print(\"Average False Positive Rate\", '{:.2%}'.format(FPRavg))\n",
    "print(\"Average False Negative Rate\", '{:.2%}'.format(FNRavg))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
